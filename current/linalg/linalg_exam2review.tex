\documentclass{report}

%subject to pathing issues, remember to change
\input{../../pkgs/preamble}
\input{../../pkgs/macros}
\input{../../pkgs/letterfonts}

\title{\Huge{Lin alg}\\ Exam 2 Review }
\author{\huge{Yan Bogdanovskyy (yawnbo)}}
\date{\today}

\begin{document}

\maketitle
\begin{multicols}{2}
\chapter{Concept review}
\section{Matrix operations to know}%
\label{sec: Matrix operations to know }
Concepts that should be known include knowing how to add, subtract and multiply matrices. Some included theory for this is that it should be know that matrix multiplication is not commutative , ie let A and B be matrices of the same size, then $ AB \neq BA $, which is important to know for the actual algebra that will be done later. \\\\
There are a few constraints for being able to add and subtract matrices, the most important of which is that they must be the same size, $ n \times m $ matrices because each term must be the sum of the corresponding terms.
\[
	\begin{bmatrix} A_{ 11 } & A_{ 12 } & \ldots &  A_{ 1n } \\ A_{ 21 } & A_{ 22 } & \ldots & A_{ 2n } \\ \vdots & \vdots & \vdots & \vdots \\ A_{ m1 } & A_{ m2 } & \ldots & A_{ mn }\end{bmatrix} \pm 
	\begin{bmatrix} B_{ 11 } & B_{ 12 } & \ldots &  B_{ 1n } \\ B_{ 21 } & B_{ 22 } & \ldots & B_{ 2n } \\ \vdots & \vdots & \vdots & \vdots \\ B_{ m1 } & B_{ m2 } & \ldots & B_{ mn }\end{bmatrix} 
.\] 
\[
= \begin{bmatrix} A_{ 11 } \pm B_{ 11 } & A_{ 12 } \pm B_{ 12 } & \ldots &  A_{ 1n } \pm B_{ 1n } \\ A_{ 21 } \pm B_{ 21 } & A_{ 22 } \pm B_{ 22 } & \ldots & A_{ 2n } \pm B_{ 2n } \\ \vdots & \vdots & \vdots & \vdots \\ A_{ m1 } \pm B_{ m1 } & A_{ m2 } \pm B_{ m2 } & \ldots & A_{ mn } \pm B_{ mn }\end{bmatrix}
.\] 
If we instead have a scalar $ c $ then our matrix will simply have each term multiplied by the scalar and doesn't rely on any size of the matrix. While we are on this topic it's important to note that there exists an identity matrix, denoted by $ I_{ n } $ where n will be the size of the columns and rows as I is always a square matrix. The identity matrix is defined as the matrix for which when multiplied by another matrix $ A $ of the same size, it will return the original matrix, ie $ AI_{ n } = A $, and looks simply like,
\[
I_{ 3 } = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}
.\] 
Now onto matrix multiplication which was mentioned above. Multiplication of matrices requires that the number of columns in the first matrix is equal to the number of rows in the second matrix, ie given a matrix $ A $ of the size $ n \times m $, our second matrix must be of size $ m \times p $ where p will be the number of columns in the resulting matrix which will be of size $ n \times p $. \\ \\ The easy way to remember this is the resulting matrix will be the unique size of each matrix, or that we will toss away the common factor between each of the matrices. \\ \\
The actual operation is done by taking the dot product, or just adding the sum of each element scaled by each other, of each row in the first matrix with each column in the second matrix, which doesn't mean that much until it's actually shown. So given two matrices $ A $ and $ B $ of the sizes mentioned above, we can write the multiplication as follows,
\[
AB = \begin{bmatrix} A_{ 11 } & A_{ 12 } & \ldots &  A_{ 1m } \\ A_{ 21 } & A_{ 22 } & \ldots & A_{ 2m } \\ \vdots & \vdots & \vdots & \vdots \\ A_{ n1 } & A_{ n2 } & \ldots & A_{ nm }\end{bmatrix} \times \begin{bmatrix} B_{ 11 } & B_{ 12 } & \ldots &  B_{ 1p } \\ B_{ 21 } & B_{ 22 } & \ldots & B_{ 2p } \\ \vdots & \vdots & \vdots & \vdots \\ B_{ m1 } & B_{ m2 } & \ldots & B_{ mp }\end{bmatrix}
.\] 
\end{multicols}
\begin{equation}
=\begin{bmatrix} A_{ 11 }B_{ 11 } + A_{ 12 }B_{ 21 } + \ldots + A_{ 1m }B_{ m1 } & A_{ 11 }B_{ 12 } + A_{ 12 }B_{ 22 } + \ldots + A_{ 1m }B_{ m2 } & \ldots & A_{ 11 }B_{ 1p } + A_{ 12 }B_{ 2p } + \ldots + A_{ 1m }B_{ mp } \\ A_{ 21 }B_{ 11 } + A_{ 22 }B_{ 21 } + \ldots + A_{ 2m }B_{ m1 } & A_{ 21 }B_{ 12 } + A_{ 22 }B_{ 22 } + \ldots + A_{ 2m }B_{ m2 } & \ldots & A_{ 21 }B_{ 1p } + A_{ 22 }B_{ 2p } + \ldots + A_{ 2m }B_{ mp } \\ \vdots & \vdots & \vdots & \vdots \\ A_{ n1 }B_{ 11 } + A_{ n2 }B_{ 21 } + \ldots + A_{ nm} B_{ m1} & A_{ n1} B_{ 12} + A_{ n2} B_{ 22} + \ldots + A_{ nm} B_{ m2} & \ldots & A_{ n1} B_{ 1p} + A_{ n2} B_{ 2p} + \ldots + A_{ nm} B _{ mp }\end{bmatrix}
\end{equation}
\begin{multicols}{2}
The above looks horrible in theory but let's do a normal example so it looks nicer, \textbf{Compute AB by the definition where they are defined as the following:}
\[
	A=\begin{bmatrix} 4 & -2 \\ -3 & 0 \\ 3 & 5 \end{bmatrix} , B=\begin{bmatrix} 1 & 3 \\ 4 & -1 \end{bmatrix}
.\] 
So let's take our first row and column and write them as vectors so we can find the dot product,
\[
A_1 = \begin{bmatrix} 4 & -2 \end{bmatrix} , B_1 = \begin{bmatrix} 1 \\ 4 \end{bmatrix}
.\] 
\[
\implies A_1 \cdot B_1 = 4 \cdot 1 + (-2) \cdot 4 = 4 - 8 = -4
.\] 
Which will become the first term $ AB_{ 11 } $ of the resulting matrix, and further terms will be written in the spots $ AB_{ jk } $ where $ j $ is the row of the first matrix and $ k $ is the column. Doing this for the rest of the terms we can find  our mat as,
\[
AB= \begin{bmatrix} -4 & 14 \\ -3 & -9 \\ 23 & 4 \end{bmatrix}
.\] 
I'm not writing out the steps for each but you get the point. TODO write it out if i feel like it.
\section{Linear transformations}%
\label{sec: Linear transformations }
Now onto linear transformations, we denote them as $ T_{ n } $ where n will be the transformation number. These transformations are functions that will take the matrix/vector $ A $ and result in a new matrix $ A_1 $, and when transformed again will result in $ A_2 $ and so on. These transformations can be known or unknown in many different problems but the main point is that because they are linear, we can write the standard matrix of T, or just the complete transform (think of this as you skip the inbetween steps and just go from $ A $ to $ A_{ n } $), as $ A_nA_{ n-1 }\ldots A_1 $. \\\\
Note that sometimes the transform is instead denoted at $ A^{ T } $ which means the same thing but in a different notation. An example from the lectures is the following,\\\\
\textbf{Given an $ m\times n $ matrix A, the transpose of A is the $ n\times m $ whose columns are formed from the corresponding rows of A. Find $ A^{ T } $ for the following matrix.} \\\\
\[
	A=\begin{bmatrix} 1 & -2 \\ 3 & 7 \\ -5 & 8 \end{bmatrix} 
.\] 
This is a simple one and we can just turn each row into the desired column to find,
\[
	A^{ T }= \begin{bmatrix} 1 & 3 & -5 \\ -2 & 7 & 8 \end{bmatrix} 
.\] 
There's more on this but I'll cover that later. 
\section{Inverse matrices and inversion}%
\label{sec: Inverse matrices and inversion }
Let's start by having a matrix $ A $ that is $ n \times n $, and is ALWAYS a square matrix, and we want to find the inverse of this matrix, denoted as $ A^{-1} $. The inverse of a matrix is defined as the matrix that when multiplied by the original matrix will result in the identity matrix, ie $ AA^{-1} = I_n $, or in this case $ A^{ -1 }A = I_n $. While you normally can't do that when multiplying, it works for this because both matrices can be considered the inverses of the other so they will always be the identity no matter what way you put it.  \\\\
First we want to even know if a matrix is invertible, which can be done by checking if the determinant of the matrix is non-zero. For a $ 2\times 2 $ matrix $ A = \begin{bmatrix} a & b \\ c & d \end{bmatrix} $, the determinant is defined as $ ad - bc $, and if this is non-zero then we can invert it. Finding the determinant of larger matrices is more complex and tedious but I'll describe it in it's own section. \\\\
For now let's focus on the ways of actually finding the inverse of a matrix. The first method will apply only to $ 2\times 2 $ matrices, and is simply,
\[
A^{ -1 } = \frac{1}{ad - bc} \begin{bmatrix} d & -b \\ -c & a \end{bmatrix}
.\] 
Which is great when you have a $ 2 \times 2 $ matrix but doesn't work for larger ones where we should instead use Gauss-Jordan elimination, which is defined as below,
\[
A_n = \begin{bmatrix} a_{ 11 } & a_{ 12 } & \ldots & a_{ 1n } \\ a_{ 21 } & a_{ 22 } & \ldots & a_{ 2n } \\ \vdots & \vdots & \ddots & \vdots \\ a_{ n1 } & a_{ n2 } & \ldots & a_{ nn }\end{bmatrix}
.\] 
\[
I_n = \begin{bmatrix} 1 & 0 & \ldots & 0 \\ 0 & 1 & \ldots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \ldots & 1 \end{bmatrix}
.\] 
\[
	A^{ -1 }_n= \begin{bmatrix} a_{ 11 } & a_{ 12 } & \ldots & a_{ 1n } & \big| & 1 & 0 & \ldots & 0 \\ a_{ 21 } & a_{ 22 } & \ldots & a_{ 2n } & \big| & 0 & 1 & \ldots & 0 \\  \vdots & \vdots & \ddots & \vdots &\bigg| & \vdots & \vdots & \ddots & \vdots \\ a_{ n1 } & a_{ n2 } & \ldots & a_{ nn } & \big| & 0 & 0 & \ldots & 1 \end{bmatrix}
.\] 
The above also looks like hell but it's really just going to be finding the RREF of the above matrix and everything to the left of the bar will be the identity matrix, but the right will be the inverse of the original matrix. In stupider terms, write the matrix as,
\[
	A^{ -1 }_{ n } = \begin{bmatrix} A & \big| & I_n \end{bmatrix} 
.\] 
Then find the RREF of the above matrix and it should look like,
\[
	\begin{bmatrix} I_n & \big| A^{ -1 } \end{bmatrix} 
.\] 
Where you have now found the inverse and can check by multiplying the original matrix by the inverse to see if you get $ I_n $. \\\\
Anyways onto the non hellish stuff. The properties of inverses are important to know too and are as follows. \\ \\
The inverse of an inverse matrix, $ \left( A^{ -1 } \right) ^{ -1 } $ is simply the original matrix $ A $, if A and B are both invertible matricies of proper sizes, then the product $ \left( AB \right) ^{ -1 } $ is invertible too and is equal to $ B^{ -1 }A^{ -1 } $. (basically just invert both and switch the multiplication order not that bad), and finally if A is invertible, then so is it's transform, $ \left( A^{ T } \right) ^{ 1 } = \left( A^{ -1 } \right) ^{ T } $. 
\section{Invertible matrices and their properties}%
\label{sec: Invertible matrices and their properties }
It's also important to know the properties of the invertible matrices, so given a matrix $ A $ of size $ n \times n $, we can say the following are either ALL true or ALL false. So if you are able to prove any one of these or disprove any one of these, you are essentially establishing if all of them are true or false. \\\\
Before the following it's important to remember that a transformation is one to one if the rank (number of non-zero rows in the RREF) is equal to the number of columns, or $ \text{rank}\left( A \right) = n $ and the transform is onto if the rank is instead equal to the number of rows, or $ \text{rank}\left( A \right) = m $. \\\\
\begin{itemize}
	\item A is an invertible matrix.
	\item A is row equivalent to $ I_n $
	\item A has n pivot positions
	\item The equation $ Ax = 0 $ has only the trivial solution. (honestly these first couple are just making sure it's invertible and has a proper RREF)
	\item The columns of A form a linearly independent set.
	\item The linear transformation $ x \mapsto Ax $ is one-to-one. (see above what that means)
	\item The equation $ Ax = b $ has at least one solution for each b in $ \mathbb{R}^n $. (i mean this is just linear dependency for $ n\times n $ but whatever)
	\item The columns of A span $ \mathbb{R}^n $. (this is just the onto part of the transform)
	\item There is an $ n \times n $ matrix C such that $ CA = I_n $. (this is literally saying that there is an inverse which is already established above I have no clue why this was put here but whatever)
	\item There is an $ n \times n $ matrix D such that $ AD = I_n $. (this is LITERALLY saying the same fuckign thing as above I don't know why i'm still writing these please don't waste time memorizing these)
	\item $ A^{ T } $ is an invertible matrix. (dude do i even say it this is clearly defined already)
	\item det $ A \neq 0 $ (i mean whatever at this point)
\end{itemize}
Half of the above are honestly just the same thing said in a different way and can be neglected as long as you know what an invertible matrix even is. I'm not doing the problems for this section because this is stupid tbh.
\section{Determinants}%
\label{sec: Determinants }
Yes my favorite part! These suck most times but are important to know. The determinant is very important for finding many things about a matrix (such as inversion) and should be known. First let's go over finding them. Given a matrix $ A $ of size $ 1 \times 1 $ the determinant is simply our one term, if A is instead a $ 2 \times 2 $ matrix then the determinant is  det $ A = a_{ 11 }a_{ 22 } - a_{ 12 }a_{ 21 } $ .\\\\
For more complex matricies there are two general ways to find the determinant, the less favorable one, in my opinion, is the Laplace expansion which is defined as follows. Given a matrix $ A $  of size $ n \times n $  we can write the determinant as, 
\[
\sum_{ j=1 } ^{ n } a_{ 1j }\left( -1 \right) ^{ 1+j }M_{ 1j }
.\] 
Which is the standard laplace expansion where $ M_{ 1j } $ is the minor of the matrix, but it's easier to just think of it as the determinant of the matrix with the first row and jth column removed, so for example if we have a $ 3 \times 3 $ matrix,
\[
A = \begin{bmatrix} a_{ 11 } & a_{ 12 } & a_{ 13 } \\ a_{ 21 } & a_{ 22 } & a_{ 23 } \\ a_{ 31 } & a_{ 32 } & a_{ 33 }\end{bmatrix}
.\] 
Then we have a minor of the matrix $ M_{ 12 } = \begin{bmatrix} a_{ 21 } & a_{ 23 } \\ a_{ 31 } & a_{ 33 }\end{bmatrix} $, because we remove the first row and second column and we can find the determinant of this matrix as $ \text{det}\left( M_{ 12 } \right) = a_{ 21 }a_{ 33 } - a_{ 23 }a_{ 31 } $. This is basically done recursively until we reach the $ 2 \times 2 $ matrix and can be done with any row so it's favored to select the row with the most zeroes. I'll give an example using the below,
\[
A= \begin{bmatrix} 1 & 2 & 3 \\ 0 & 4 & 5 \\ 6 & 7 & 8 \end{bmatrix}
.\] 
So step one is to just choose a row and column. This is easiest when we have a zero somewhere so our a term will cancel. Let's use the first column and second row. We can write the determinant as,
\[
\text{det}\left( A \right) = 1\left( -1 \right) ^{ 1+1 }M_{ 11 } + 0\left( -1 \right) ^{ 2+1 }M_{ 21 } + 6\left( -1 \right) ^{ 3+1 }M_{ 31 }
.\] 
Where we strikeout our rows and find our minors and solve their det's,
\[
M_{ 11 } = \begin{bmatrix} 4 & 5 \\ 7 & 8 \end{bmatrix} , M_{ 21 } = \begin{bmatrix} 2 & 3 \\ 7 & 8 \end{bmatrix} , M_{ 31 } = \begin{bmatrix} 2 & 3 \\ 4 & 5 \end{bmatrix}
.\] 
\[
\text{det}\left( A \right) = 1\left( -1 \right) ^{ 1+1 }\left( 4 \cdot 8 - 5 \cdot 7 \right) + 0 + 6\left( -1 \right) ^{ 3+1 }\left( 2 \cdot 5 - 3 \cdot 4 \right) 
.\] 
\[
= 1\left( -1 \right) ^{ 2 }\left( 32 - 35 \right) + 0 + 6\left( -1 \right) ^{ 4 }\left( 10 - 12 \right) 
.\] 
\[
= 1\left( -3 \right) + 0 + 6\left( -2 \right) = -3 + 0 - 12 = -15
.\] 
This method has tons of arithmetic and sucks ass so let's instead us the other method where we just row reduce our matrix. Using the same matrix and basic matrix operations we can do the following,
\[
	A \to \begin{bmatrix} 1 & 2 & 3 \\ 0 & 4 & 5 \\ 0 & -5 & -10 \end{bmatrix} \to \begin{bmatrix} 1 & 2 & 3 \\ 0 & 4 & 5 \\ 0 & 0 & -\frac{ 15 }{ 4 }  \end{bmatrix} 
.\] 
And can now find our determinant as 
\[
\text{det}A = A_{ 11 }\cdot A_{ 22 }\cdot A_{ 33 }\cdot \ldots * A_{ nn } 
.\] 
Or in this case,
\[
\text{det}A= 1\cdot 4\cdot -\frac{ 15 }{ 4 } = -15
.\] 
This method is 10x simpler than the last and I would hope the test doesn't ask you to use the other. This method just simply eliminates all numbers below the $ A_{ ii } $th element and then multiplies the diagonal. Now onto the properties of these, which will probably also be half useless,
\begin{itemize}
	\item det A = det $ A^{ T } $ (just saying that linear transformations give the same determinant)
	\item det $ KA $ = $ K^{ n } $det A for any scalar $ K $. This one is actually kinda useful so I would remember it. 
	\item A is invertible if and only if det A $ \neq 0 $.
	\item If B is also an $ n \times n $ matrix, then det $ AB = $ det A $\cdot$ det B. (this is just the property of multiplication but it's helpful)
	\item If A is invertible, then det $ A^{ -1 } $ = $ \frac{ 1 }{ \text{det }A }  $
\end{itemize}
That's about it for determinants, but practice problems are nice to look at and practice although I won't be showing them here, but the review says 3.1 example 2-4 and 3.2 example 2-4. imo the pearson is just as good for this though so i would use that too. Also look up determinant calculator and the first result has great explanations for the second (simpler) method instead of the disgusting laplace expansion.
\section{Subspaces}%
\label{sec: Subspaces }
Let's start with H, a subset of the vector space V. We need ways to determine if H is actuall in the subspace of V or not, so we can use the below methods. \\\\
Before that I want to define what a subspace actually is tho. It is defined as some subset of the space that is itself a vector space under the same operations of addition and scaling as V. So if we have a vector space $ V $ and a subset $ H \subseteq V $, then H is a subspace of V if the following are true,
\begin{itemize}
	\item Firstly, if the zero vector of V is in H (ie $ 0 \in H $). This is important because the zero vector is the identity for addition and must be in any vector space.
	\item Secondly, if for all values of u, $ v \in H $ or the sum $ u+v \in H $. 
	\itme and finally if it's closed under scalar multiplication. This means that for all $ v \in H $ and all scalars $ c \in \mathbb{R} $, the product $ cv \in H $.
\end{itemize}
The above may seem kind of confused but basically think of it as a smaller vector space inside of another bigger one (v). The requirements are just that it contins the orign and is closed under the basic operations of addition and multiplication. That's it. \\\\
As a simple example, think of our original space (v) as $ \mathbb{R}^{ 3 } $ and the subspace (H) as the subspace of the x axis of form $ \left( x,0,0 \right)  $. Checking our properties, we can see that the zero vector $ \left( 0,0,0 \right)  $ is in H, and that if we take two vectors in H, say $ \left( x_1,0,0 \right)  $ and $ \left( x_2,0,0 \right)  $, then their sum is also in H as $ \left( x_1+x_2,0,0 \right)  $. Finally if we take a scalar $ c $ and multiply it by a vector in H, say $ c\left( x,0,0 \right)  = \left( cx,0,0 \right)  $, then this is also in H. So we can conclude that H is a subspace of V. \\\\ 
The concept review also gives the example of if $ v_1, \ldots v_n $ are vectors in V and H = Span $ \left( v_1,\ldots,v_n \right)  $ then H is a subspace of V. This is because the span of a set of vectors is the set of all linear combinations of those vectors, and since linear combinations are closed under addition and scalar multiplication, the span will always be a subspace. \\\\
Some more confusing ones are that if H is a subset of $ \mathbb{R}^{ n } $ and A is an $ m \times n $ matrix, then if H = Null(A) then H is a subspace of $ \mathbb{R}^{ n } $. This is because the null space of a matrix is the set of all vectors that when multiplied by the matrix result in the zero vector, and since it contains the zero vector and is closed under addition and scalar multiplication, it is a subspace. \\\\
Similarly, if we let H be a subset of $ \mathbb{R}^{ m } $ and again let A be an $ m \times n $ matrix, then if H = Col(A) then H is a subspace of $ \mathbb{R}^{ m } $. This also seems confusing but just know that if you have the column space of a matrix, then it is the set of all linear combinations of the columns of the matrix, which in turn means that it is closed under addition and scalar multiplication, and can create the zero vector which matches all of our definitions. \\\\
\section{Null space, Column space, Row space}%
\label{sec: Null space, Column space, Row space }
This stuff is mostly review and just to be certain of definitions for the rest but I'll go over it anyways. For all the below let A be an $ m \times n $ matrix. We can write the null space of Nul(A) which will just be the set of solutions for the equation $ A \vec{ x }= 0 $. For a linearly independent set we will have only the trivial solution but this will change for dependent sets. The review gives the equation
\[
\text{Nul}( A ) = \left\{ \vec{ x } \in \mathbb{ R }^{ n } : A \vec{ x } = 0 \right\}
.\] 
Which looks bad but just says that if x is a vector in $ \mathbb{R}^{ n } $ and it makes the zero vector then we are in the null space of that set of vectors. \\\\
The \textbf{Column space} of A is written as Col(A) and is defined as the set of linear combinations of the columns of A. The equation,
\[
\text{Col}( A ) = \left\{ \vec{ b } \in \mathbb{ R }^{ m } : \vec{ b } = A \vec{ x } \text{ for some } \vec{ x } \in \mathbb{ R }^{ n } \right\}
.\] 
is just saying that if we have a vector $ \vec{ b } $ then we can make it with a linear combination of the columns of A, or in other words, if we can write $ \vec{ b } = A \vec{ x } $ for some vector $ \vec{ x } $ then we are in the column space of A. \\\\
The \textbf{row space} of A is written as Row(A) and is just the set of all linear combinations of the row vectors. This is also a subset of $ \mathbb{R}^{ n } $. \\\\
The useful summaries of each of these is as follows,
\begin{itemize}
	\item The null space is just the set of solutions to $ A \vec{ x } = 0 $ written in parametric form, then the set forms Null(A).
	\item The pivot columns of A form the basis for the column space of A, and the span of these columns is the column space.
	\item Finally, the nonzero rows of R form a basis for Row(A). 
\end{itemize}
I'm not doing review questions for any of this but just review the solutions and questions for the given practice test and you'll be fine. Good luck on exam 2 and make sure to watch reels every 30 minutes to stay focused!
\end{multicols}
\end{document}
